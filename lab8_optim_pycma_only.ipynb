{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - CEM\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2021/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2021](https://moodle.polytechnique.fr/course/view.php?id=9352) Lab session #8\n",
    "\n",
    "2019-2021 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2021/master?filepath=lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2021/raw/master/lab8_optim_cem_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym[box2d] numpy pandas seaborn imageio\n",
    "``\n",
    "\n",
    "C.f. https://github.com/openai/gym#installing-everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install inf581\".split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "    from inf581 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from inf581 import lab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a policy for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Parametric Stochastic Policy ################################################\n",
    "###############################################################################\n",
    "\n",
    "# Activation functions ########################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    x_and_zeros = np.array([x, np.zeros(x.shape)])\n",
    "    return np.max(x_and_zeros, axis=0)\n",
    "\n",
    "# Dense Multi-Layer Neural Network ############################################\n",
    "\n",
    "class NeuralNetworkPolicy:\n",
    "\n",
    "    def __init__(self, activation_functions, shape_list):\n",
    "        self.activation_functions = activation_functions\n",
    "        self.shape_list = shape_list\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        weights = unflatten_weights(theta, self.shape_list)\n",
    "\n",
    "        return feed_forward(inputs=state,\n",
    "                            weights=weights,\n",
    "                            activation_functions=self.activation_functions)\n",
    "\n",
    "\n",
    "def feed_forward(inputs, weights, activation_functions, verbose=False):\n",
    "    x = inputs.copy()\n",
    "    for layer_weights, layer_activation_fn in zip(weights, activation_functions):\n",
    "\n",
    "        y = np.dot(x, layer_weights[1:])\n",
    "        y += layer_weights[0]\n",
    "        \n",
    "        layer_output = layer_activation_fn(y)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"x\", x)\n",
    "            print(\"bias\", layer_weights[0])\n",
    "            print(\"W\", layer_weights[1:])\n",
    "            print(\"y\", y)\n",
    "            print(\"z\", layer_output)\n",
    "\n",
    "        x = layer_output\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def weights_shape(weights):\n",
    "    return [weights_array.shape for weights_array in weights]\n",
    "\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\"Convert weight parameters to a 1 dimension array (more convenient for optimization algorithms)\"\"\"\n",
    "    nested_list = [weights_2d_array.flatten().tolist() for weights_2d_array in weights]\n",
    "    flat_list = list(itertools.chain(*nested_list))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def unflatten_weights(flat_list, shape_list):\n",
    "    \"\"\"The reverse function of `flatten_weights`\"\"\"\n",
    "    length_list = [shape[0] * shape[1] for shape in shape_list]\n",
    "\n",
    "    nested_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    for length, shape in zip(length_list, shape_list):\n",
    "        nested_list.append(np.array(flat_list[start_index:start_index+length]).reshape(shape))\n",
    "        start_index += length\n",
    "\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Objective function ##########################################################\n",
    "###############################################################################\n",
    "\n",
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, env, policy, ndim, num_episodes=1, max_time_steps=float('inf'), minimization_solver=False):\n",
    "        self.ndim = ndim\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "        self.hist = []\n",
    "        self.hist_policy = []\n",
    "\n",
    "        \n",
    "    def eval(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        \"\"\"Evaluate a policy\"\"\"\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            total_rewards = 0.\n",
    "            state = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                if render:\n",
    "                    self.env.render_wrapper.render()\n",
    "\n",
    "                action = self.policy(state, policy_params)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "            if render:\n",
    "                print(\"Test Episode {0}: Total Reward = {1}\".format(i_episode, total_rewards))\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.\n",
    "\n",
    "        return average_total_rewards   # Optimizers do minimization by default...\n",
    "\n",
    "    \n",
    "    def __call__(self, policy_params):\n",
    "        return self.eval(policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: solve the LunarLander problem (continuous version) with CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "VERBOSE = False\n",
    "LOG_SCORES = True\n",
    "LOG_POLICIES = False\n",
    "LOG_RECORD_INTERVAL = 1000   # TODO\n",
    "\n",
    "GYM_ENVIRONMENT = \"LunarLanderContinuous-v2\"\n",
    "\n",
    "###############################################################################\n",
    "# Main ########################################################################\n",
    "###############################################################################\n",
    "\n",
    "env = gym.make(GYM_ENVIRONMENT)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation_space_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(observation_space_dim)\n",
    "init_mean_array = np.zeros(178)\n",
    "init_var_array = np.ones(178) * 1000.\n",
    "\n",
    "# Make a neural network with 1 hidden layer of 16 units\n",
    "h_size = 16     # Number of neurons on the hidden layer\n",
    "weights = (np.zeros([env.observation_space.shape[0] + 1, h_size]),\n",
    "           np.zeros([h_size + 1, env.action_space.shape[0]]))\n",
    "\n",
    "# Set the neural network activation functions (one function per layer)\n",
    "activation_functions = (relu, tanh)\n",
    "\n",
    "flat_weights_list = flatten_weights(weights)\n",
    "num_params = len(flat_weights_list)\n",
    "print(\"Number of parameters (neural network weights) to optimize:\", num_params)\n",
    "w_shape = weights_shape(weights)\n",
    "print(\"Number of parameters per layer:\", w_shape)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(activation_functions, w_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       ndim=num_params,  # number of dimensions of the parameter (weights) space\n",
    "                                       num_episodes=1,   # <- resampling\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 2: test the CMAES algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCMA: Python implementation of CMA-ES (from Nikolaus Hansen).\n",
    "\n",
    "Source code:\n",
    "\n",
    "- http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#python\n",
    "- https://github.com/CMA-ES/pycma\n",
    "- https://pypi.org/project/cma/\n",
    "\n",
    "Official documentation:\n",
    "\n",
    "- http://cma.gforge.inria.fr/apidocs-pycma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       ndim=num_params,  # number of dimensions of the parameter (weights) space\n",
    "                                       num_episodes=1,\n",
    "                                       max_time_steps=500,\n",
    "                                       minimization_solver=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_optimal, es = cma.fmin2(objective_function, x0=np.random.random(num_params), sigma0=10., options={'maxfevals': 1500})\n",
    "theta = x_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "plt.rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "cma.plot();  # shortcut for es.logger.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final policy\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"ex_pycma_ll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
