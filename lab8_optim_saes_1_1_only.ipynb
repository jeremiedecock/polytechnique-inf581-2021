{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - CEM\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2021/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2021](https://moodle.polytechnique.fr/course/view.php?id=9352) Lab session #8\n",
    "\n",
    "2019-2021 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2021/master?filepath=lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2021/raw/master/lab8_optim_cem_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym[box2d] numpy pandas seaborn imageio\n",
    "``\n",
    "\n",
    "C.f. https://github.com/openai/gym#installing-everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install inf581\".split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "    from inf581 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from inf581.saesmod import saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: implement SAES and solve the CartPole problem (continuous version) with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric Stochastic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    return prob_push_right\n",
    "\n",
    "\n",
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "    r = np.random.rand()\n",
    "    return 1 if r < prob_push_right else 0\n",
    "\n",
    "\n",
    "# Logistic Regression ############################################\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, env):        \n",
    "        self.num_params = env.observation_space.shape[0]\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        return draw_action(state, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, env, policy, num_episodes=1, max_time_steps=float('inf'), minimization_solver=True):\n",
    "        self.ndim = policy.num_params  # Number of dimensions of the parameter (weights) space\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "        \n",
    "    def eval(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        \"\"\"Evaluate a policy\"\"\"\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            total_rewards = 0.\n",
    "            state = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                if render:\n",
    "                    self.env.render_wrapper.render()\n",
    "\n",
    "                action = self.policy(state, policy_params)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "            if render:\n",
    "                print(\"Test Episode {0}: Total Reward = {1}\".format(i_episode, total_rewards))\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.\n",
    "\n",
    "        return average_total_rewards   # Optimizers do minimization by default...\n",
    "\n",
    "    \n",
    "    def __call__(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        return self.eval(policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `cem_uncorrelated` function that search the best $\\theta$ parameters with a Cross Entropy Method. Use the parametric policy defined in the previous exercise.\n",
    "$\\mathbb{P}$ can be defined as an isotropic normal distribution with a constant standard deviation of 0.5. In this case, $\\theta$ only contains the mean of the normal distribution $\\mathbb{P}$. We recommend to use the following setup: $m = 50$ and $m_{\\text{elite}} = 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1+1)-SA-ES**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{x}$: initial solution<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $\\tau$: self-adaptation learning rate<br>\n",
    "\n",
    "**FOR EACH** generation<br>\n",
    "$\\quad\\quad$ 1. mutation of $\\sigma$ (current individual strategy) : $\\sigma' \\leftarrow \\sigma ~ e^{\\tau \\mathcal{N}(0,1)}$<br>\n",
    "$\\quad\\quad$ 2. mutation of $\\boldsymbol{x}$ (current solution) : $\\boldsymbol{x}' \\leftarrow \\boldsymbol{x} + \\sigma' ~ \\mathcal{N}(0,1)$<br>\n",
    "$\\quad\\quad$ 3. eval $f(\\boldsymbol{x}')$<br>\n",
    "$\\quad\\quad$ 4. survivor selection $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}'$ and $\\sigma \\leftarrow \\sigma'$ if $f(\\boldsymbol{x}') \\leq f(\\boldsymbol{x})$<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saes_1_1(objective_function,\n",
    "             x_array,\n",
    "             sigma_array,\n",
    "             max_iterations=500,\n",
    "             tau=None,\n",
    "             print_every=10,\n",
    "             success_score=float(\"inf\"),\n",
    "             num_evals_for_stop=None,\n",
    "             hist_dict=None):\n",
    "\n",
    "    # Number of dimension of the solution space\n",
    "    d = x_array.shape[0]\n",
    "\n",
    "    if tau is None:\n",
    "        # Self-adaptation learning rate\n",
    "        tau = 1./(2.*d)\n",
    "\n",
    "    score = objective_function(x_array)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "\n",
    "        # 1. Mutation of sigma (current \"individual strategy\")\n",
    "        new_sigma_array = sigma_array * np.exp(tau * np.random.normal(size=d))\n",
    "\n",
    "        # 2. Mutation of x (current solution)\n",
    "        new_x_array = x_array + new_sigma_array * np.random.normal(size=d)\n",
    "\n",
    "        # 3. Eval f(x')\n",
    "        new_score = objective_function(new_x_array)\n",
    "\n",
    "        # 4. survivor selection (we follow the ES convention and do minimization)\n",
    "        if new_score <= score:      # You may try `new_score < score` for less exploration\n",
    "            score = new_score\n",
    "            x_array = new_x_array\n",
    "            sigma_array = new_sigma_array\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "        \n",
    "        if iteration_index % print_every == 0:\n",
    "            #print(\"Iteration {}\\tScore {}\\tMean: {}\\tVar: {}\".format(iteration_index, score_mean, str(mean_array), str(var_array)))\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "        \n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [score] + x_array.tolist() + sigma_array.tolist()\n",
    "\n",
    "        # STOPING CRITERIA ####################################################\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(new_x_array, num_episodes=num_evals_for_stop)\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduce bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "                \n",
    "    return x_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=900,\n",
    "                                       minimization_solver=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 tau = 0.001,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=100,\n",
    "                 success_score=-200,\n",
    "                 num_evals_for_stop=100,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"ex3_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a parametric policy $\\pi_\\theta$ for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve problems having a continuous space, especially to solve the LunarLander problem in the next exercise, we need to define and implement an appropriate parametric policy.\n",
    "For this purpose, we recommend the following neural network:\n",
    "- one hidden layer of 16 units having a ReLu activation function\n",
    "- a tanh activation function on the output layer (be careful on the number of output units)\n",
    "\n",
    "To solve environments with continuous action space like LunarLander with Direct Policy Search methods, a simple procedure that compute the feed forward signal is needed (we don't do back propagation here).\n",
    "A procedure to set/get weights of the network from/to a single vector $\\theta$ will also be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions ########################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    x_and_zeros = np.array([x, np.zeros(x.shape)])\n",
    "    return np.max(x_and_zeros, axis=0)\n",
    "\n",
    "# Dense Multi-Layer Neural Network ############################################\n",
    "\n",
    "class NeuralNetworkPolicy:\n",
    "\n",
    "    def __init__(self, env, h_size=16):   # h_size = number of neurons on the hidden layer\n",
    "        # Set the neural network activation functions (one function per layer)\n",
    "        self.activation_functions = (relu, tanh)\n",
    "        \n",
    "        # Make a neural network with 1 hidden layer of `h_size` units\n",
    "        weights = (np.zeros([env.observation_space.shape[0] + 1, h_size]),\n",
    "                   np.zeros([h_size + 1, env.action_space.shape[0]]))\n",
    "\n",
    "        self.shape_list = weights_shape(weights)\n",
    "        print(\"Number of parameters per layer:\", self.shape_list)\n",
    "        \n",
    "        self.num_params = len(flatten_weights(weights))\n",
    "        print(\"Number of parameters (neural network weights) to optimize:\", self.num_params)\n",
    "\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        weights = unflatten_weights(theta, self.shape_list)\n",
    "\n",
    "        return feed_forward(inputs=state,\n",
    "                            weights=weights,\n",
    "                            activation_functions=self.activation_functions)\n",
    "\n",
    "\n",
    "def feed_forward(inputs, weights, activation_functions, verbose=False):\n",
    "    x = inputs.copy()\n",
    "    for layer_weights, layer_activation_fn in zip(weights, activation_functions):\n",
    "\n",
    "        y = np.dot(x, layer_weights[1:])\n",
    "        y += layer_weights[0]\n",
    "        \n",
    "        layer_output = layer_activation_fn(y)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"x\", x)\n",
    "            print(\"bias\", layer_weights[0])\n",
    "            print(\"W\", layer_weights[1:])\n",
    "            print(\"y\", y)\n",
    "            print(\"z\", layer_output)\n",
    "\n",
    "        x = layer_output\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def weights_shape(weights):\n",
    "    return [weights_array.shape for weights_array in weights]\n",
    "\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\"Convert weight parameters to a 1 dimension array (more convenient for optimization algorithms)\"\"\"\n",
    "    nested_list = [weights_2d_array.flatten().tolist() for weights_2d_array in weights]\n",
    "    flat_list = list(itertools.chain(*nested_list))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def unflatten_weights(flat_list, shape_list):\n",
    "    \"\"\"The reverse function of `flatten_weights`\"\"\"\n",
    "    length_list = [shape[0] * shape[1] for shape in shape_list]\n",
    "\n",
    "    nested_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    for length, shape in zip(length_list, shape_list):\n",
    "        nested_list.append(np.array(flat_list[start_index:start_index+length]).reshape(shape))\n",
    "        start_index += length\n",
    "\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: solve the LunarLander problem (continuous version) with CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** read https://gym.openai.com/envs/LunarLanderContinuous-v2/ and https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2 to discover the LunarLanderContinuous environment.\n",
    "\n",
    "**Notice:** A reminder of Gym main concepts is available at https://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the MountainCar environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    #action = np.array([1., 1.])\n",
    "    action = np.array([-1., -1.])\n",
    "    #action = env.action_space.sample()   # Random policy\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex3_explore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,     # <- resampling\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.    # 1000.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=10,\n",
    "                 success_score=-200,\n",
    "                 num_evals_for_stop=None,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "ax = df.iloc[:,0].plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"ex2_ll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
