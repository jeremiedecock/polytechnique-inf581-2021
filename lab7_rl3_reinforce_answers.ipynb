{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Policy Gradient\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2021/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2021](https://moodle.polytechnique.fr/course/view.php?id=9352) Lab session #7\n",
    "\n",
    "2019-2021 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab7_rl3_reinforce_answers.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2021/master?filepath=lab7_rl3_reinforce_answers.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab7_rl3_reinforce_answers.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2021/raw/master/lab7_rl3_reinforce_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a previous lab, we have dealt with reinforcement learning in discrete state and action spaces. To do so, we used methods based on value function and, especially, $Q$-function estimation. The $Q$-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and $Q$-Learning). \n",
    "\n",
    "Yet, these methods do not scale to large state spaces and especially not to the case of continuous state spaces. To address these issues one can either extend value-based methods making use of value-function approximation or directly search in policy spaces. In this lab, we will explore the second solution. \n",
    "\n",
    "More specifically, we will search in a family of parameterized policies $\\pi_\\theta(s, a)$ using a policy gradient method. This method performs gradient ascent in the policy space so that the total return is maximized. We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $H$, called horizon. \n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\E_{\\pi_\\theta}\\left[\\sum_{t=1}^H r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\E_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(s,.)$ and $H$ is the horizon of the episode. In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$. The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "With \n",
    "\n",
    "$$Q^\\pi(s,a) = \\E^\\pi \\left[\\sum_{t=1}^H r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "Policy Gradient theorem is extremely powerful because it says one doesn't need to know the dynamics of the system to compute the gradient if one can compute the $Q$-function of the current policy. By applying the policy and observing the one-step transitions is enough. Using a stochastic gradient ascent and replacing $Q^{\\pi_\\theta}(s_t,a_t)$ by a Monte Carlo estimate $R_t = \\sum_{t'=t}^H r(s_{t'},a_{t'})$ over one single trajectory, we end up with a special case of the REINFORCE algorithm (see Algorithm below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "REINFORCE with Policy Gradient theorem Algorithm\n",
    "---\n",
    "\n",
    "Initialize $\\theta^0$ as random<br>\n",
    "Initialize step-size $\\alpha_0$<br>\n",
    "$n \\leftarrow 0$<br>\n",
    "<b>WHILE</b> no convergence<br>\n",
    "\t$\\quad$ Generate rollout $h_n \\leftarrow \\{s_1^n,a_1^n,r_1^n, \\ldots, s_H^n, a_H^n, r_H^n\\} \\sim \\pi_{\\theta^n}$<br>\n",
    "\t$\\quad$ $PG_\\theta \\leftarrow 0$<br>\n",
    "\t$\\quad$ <b>FOR</b> $t=1$ to $H$<br>\n",
    "\t\t$\\quad\\quad$ $R_t \\leftarrow \\sum_{t'=t}^H r_{t'}^n$<br>\n",
    "\t\t$\\quad\\quad$ $PG_\\theta \\leftarrow PG_\\theta + \\nabla_\\theta \\log \\pi_{\\theta^{n}}(s_t,a_t) ~ R_t$<br>\n",
    "\t$\\quad$ $n \\leftarrow n + 1$ <br>\n",
    "\t$\\quad$ $\\theta^n \\leftarrow \\theta^{n-1} + \\alpha_n PG_\\theta$<br>\n",
    "\t$\\quad$  update $\\alpha_n$ (if step-size scheduling)<br>\n",
    "\n",
    "<b>RETURN</b> $\\theta^n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: by replacing the $Q$-function by a Monte-Carlo estimate, we get rid of the Markov assumption and this algorithm is expected to work even in non-Markovian systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by OpenAI Gym suite.\n",
    "OpenAI Gym provides controllable environments (https://gym.openai.com/envs/) for research in reinforcement learning.\n",
    "Especially, we will try to solve the CartPole-v0 environment (https://gym.openai.com/envs/CartPole-v0/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 195 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: {LEFT, RIGHT}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cell does this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym numpy pandas seaborn imageio\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_requirements = [\n",
    "    \"gym\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"seaborn\",\n",
    "    \"pyvirtualdisplay\",\n",
    "    \"imageio\"\n",
    "]\n",
    "\n",
    "debian_packages = [\n",
    "    \"xvfb\",\n",
    "    \"x11-utils\"\n",
    "]\n",
    "\n",
    "if is_colab():\n",
    "\n",
    "    def run_subprocess_command(cmd):\n",
    "        # run the command\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "        # print the output\n",
    "        for line in process.stdout:\n",
    "            print(line.decode().strip())\n",
    "\n",
    "    for i in colab_requirements:\n",
    "        run_subprocess_command(\"pip install \" + i)\n",
    "\n",
    "    for i in debian_packages:\n",
    "        run_subprocess_command(\"apt install \" + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup virtual display for Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can uncomment the following cell to install required packages in your local environment (remove only the `#` not the `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import IPython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render(mode='rgb_array')\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            imageio.mimsave(filename + '.gif', [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29)\n",
    "            return Image(open(filename + '.gif','rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hands on Cart Pole\n",
    "\n",
    "As for the previous lab, open the \\texttt{test\\_envs.py} file (lab materials) and run it to get used to basic functions on Cart Pole. Identify the action selection command in the code. You can notice it's not different from previously. Some more functions are available for multidimensional continuous states. \n",
    "\n",
    "The most important command is still the \\texttt{env.step(action)} one. It applies the selected action to the environment and returns an observation (next state), a reward, a flag that is set to $True$ if the episode has terminated and some info. \n",
    "\n",
    "Try to use a different policy (for instance, a constant action) to understand the role of that command. Although the Cart Pole has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some info about the environment\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "\n",
    "    for t in range(200):\n",
    "        env.render_wrapper.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a sigmoid policy\n",
    "\n",
    "As the number of actions is $2$, one can see the problem of controlling the Cart Pole as a binary classification problem.\n",
    "Binary classification can be easily solved thanks to logistic regression which transforms the classification problem into a regression problem using the sigmoid function defined by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$\n",
    "\n",
    "**To do**:\n",
    "- Build a policy that select the *RIGHT* action with probability $\\sigma(\\theta^\\top s)$, where $\\theta$ is the parameter vector and $s$ is the 4-dimension state vector. \n",
    "- As a measure of performance, count the average return (sum or rewards) over 100 episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute $\\nabla_\\theta \\log \\pi_\\theta (s,a)$\n",
    "\n",
    "Verify that, for a sigmoid policy:\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT}) = \\pi_\\theta (s, \\text{LEFT}) \\times s$\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT}) = - \\pi_\\theta (s, \\text{RIGHT}) \\times s$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement REINFORCE\n",
    "\n",
    "To implement REINFORCE, you will need to follow these steps:\n",
    "- Run rollouts with current policy $\\pi_\\theta(s,a)$ with fixed horizon $H$. Since the goal is to attain an average return of 195, horizon should be larger than 195 steps (say 250 for instance).\n",
    "- Compute policy gradient = $\\sum_{t=1}^H \\nabla_\\theta \\log \\pi_\\theta(s_t,a,_t) R_t$.\n",
    "- Update parameters with gradient ascent.\n",
    "- Verify if the new policy meets success requirement (average return $>$194)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v0\"\n",
    "EPISODE_DURATION = 300\n",
    "ALPHA_INIT = 0.1\n",
    "SCORE = 195.0\n",
    "TEST_TIME = 100\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute policy parameterisation\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return policy\n",
    "def get_policy(s, theta):\n",
    "\n",
    "    p_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    pi = [1-p_right, p_right]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw an action according to current policy\n",
    "def act_with_policy(s, theta):\n",
    "    p_right = get_policy(s, theta)[1]\n",
    "    r = np.random.rand()\n",
    "    if r < p_right:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode\n",
    "def gen_rollout(env, theta, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    s_t = env.reset()\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(s_t)\n",
    "\n",
    "    for t in range(max_episode_length):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "        a_t = act_with_policy(s_t, theta)\n",
    "        s_t, r_t, done, info = env.step(a_t)\n",
    "        episode_states.append(s_t)\n",
    "        episode_actions.append(a_t)\n",
    "        episode_rewards.append(r_t)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, theta, score = SCORE, num_episodes = TEST_TIME , max_episode_length=EPISODE_DURATION, render=False):\n",
    "    num_success = 0\n",
    "    average_return = 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        _, _, episode_rewards = gen_rollout(env, theta, max_episode_length, render)\n",
    "\n",
    "        total_rewards = sum(episode_rewards)\n",
    "\n",
    "        if total_rewards > score:\n",
    "            num_success+=1\n",
    "\n",
    "        average_return += (1.0 / num_episodes) * total_rewards\n",
    "\n",
    "        if render:\n",
    "            print(\"Test Episode {0}: Total Reward = {1} - Success = {2}\".format(i_episode,total_rewards,total_rewards>score))\n",
    "\n",
    "    if average_return > score:\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "\n",
    "    return success, num_success, average_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Policy Gradient for a given episode\n",
    "def compute_PG(episode_states, episode_actions, episode_rewards, theta):\n",
    "\n",
    "    H = len(episode_rewards)\n",
    "    PG = 0\n",
    "\n",
    "    for t in range(H):\n",
    "\n",
    "        pi = get_policy(episode_states[t], theta)\n",
    "        a_t = episode_actions[t]\n",
    "        R_t = sum(episode_rewards[t::])\n",
    "        if a_t == LEFT:\n",
    "            g_theta_log_pi = - pi[RIGHT] * episode_states[t] * R_t\n",
    "        else:\n",
    "            g_theta_log_pi = pi[LEFT] * episode_states[t] * R_t\n",
    "        \n",
    "        PG += g_theta_log_pi\n",
    "\n",
    "    return PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train until average return is larger than SCORE\n",
    "def train(env, theta_init, max_episode_length = EPISODE_DURATION, alpha_init = ALPHA_INIT):\n",
    "\n",
    "    theta = theta_init\n",
    "    i_episode = 0\n",
    "    average_returns = []\n",
    "\n",
    "    success, _, R = test_policy(env, theta)\n",
    "    average_returns.append(R)\n",
    "\n",
    "    # Train until success\n",
    "    while (not success):\n",
    "\n",
    "        # Rollout\n",
    "        episode_states, episode_actions, episode_rewards = gen_rollout(env, theta, max_episode_length)\n",
    "\n",
    "        # Schedule step size\n",
    "        #alpha = alpha_init\n",
    "        alpha = alpha_init / (1 + i_episode)\n",
    "\n",
    "        # Compute gradient\n",
    "        PG = compute_PG(episode_states, episode_actions, episode_rewards, theta)\n",
    "\n",
    "        # Do gradient ascent\n",
    "        theta += alpha * PG\n",
    "\n",
    "        # Test new policy\n",
    "        success,_,R = test_policy(env, theta, render=False)\n",
    "\n",
    "        # Monitoring\n",
    "        average_returns.append(R)\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\"Episode {0}, average return: {1}\".format(i_episode, R))\n",
    "\n",
    "    return theta, i_episode, average_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(ENV_NAME)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "#env.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(1, dim)\n",
    "\n",
    "# Train agent\n",
    "theta, i, average_returns = train(env, theta_init)\n",
    "\n",
    "print(\"Solved after {} iterations\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final policy\n",
    "test_policy(env, theta, num_episodes=10, render=True)\n",
    "env.render_wrapper.make_gif(\"ex4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curve\n",
    "plt.plot(range(len(average_returns)),average_returns)\n",
    "plt.title(\"Average reward on 100 episodes\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = np.array([0.1, 0.1, 0.1, 0.9])\n",
    "#theta = np.array([0.1, 0.9, 0.1, 0.9])\n",
    "#print(get_policy(state, theta))\n",
    "#print(type(get_policy(state, theta)))\n",
    "#\n",
    "#action_list = [act_with_policy(state, theta) for _ in range(2000)]\n",
    "##print(action_list)\n",
    "#plt.hist(action_list)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
