{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - CEM\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2021/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2021](https://moodle.polytechnique.fr/course/view.php?id=9352) Lab session #8\n",
    "\n",
    "2019-2021 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2021/master?filepath=lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2021/raw/master/lab8_optim_cem_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym numpy pandas seaborn imageio\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install inf581\".split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "    from inf581 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like CEM are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for previous Reinforcement Learning labs, we will use standard problems provided by OpenAI Gym suite.\n",
    "Especially, we will try to solve the MountainCarContinuous-v0 problem (\\url{https://github.com/openai/gym/wiki/MountainCarContinuous-v0}) which offers both continuous space and action states.\n",
    "\n",
    "An underpowered car must climb a one-dimensional hill to reach a target.\n",
    "The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates.\n",
    "On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty. The problem is considered solved if a reward of 90 is obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hands on the mountain car environment\n",
    "\n",
    "**Task 1:** read  https://gym.openai.com/envs/CartPole-v0/ and https://github.com/openai/gym/wiki/CartPole-v0 to discover the CartPole environment.\n",
    "\n",
    "**Notice:** A reminder of Gym main concepts is available at https://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = 0\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = 1\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a sigmoid policy\n",
    "\n",
    "As the number of actions is $2$ (push the cart to the left or push it to the right), one can see the problem of controlling the Cart Pole as a binary classification problem.\n",
    "Binary classification can be easily solved thanks to logistic regression which transforms the classification problem into a regression problem using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `sigmoid` function defined by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Complete the `logistic_regression` function that implements the logistic regression. This function returns the probability to draw action 1 (\"push right\") w.r.t the parameter vector $\\theta$ and the input vector $s$ (the 4-dimension state vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_regression_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(s, theta):\n",
    "    ### BEGIN SOLUTION ###\n",
    "    prob_push_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return prob_push_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Complete the `draw_action` function that draw an action according to current policy i.e. that select the *RIGHT* action with probability $\\sigma(\\theta^\\top s)$, where $\\theta$ is the parameter vector and $s$ is the 4-dimension state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    r = np.random.rand()\n",
    "    if r < prob_push_right:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute $\\nabla_{\\theta} \\log \\pi_\\theta (s,a)$\n",
    "\n",
    "Verify that, for a sigmoid policy:\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT}) = \\pi_\\theta (s, \\text{LEFT}) \\times s$\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT}) = - \\pi_\\theta (s, \\text{RIGHT}) \\times s$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "First, let's define $\\pi(s, RIGHT)$ and $\\pi(s, LEFT)$:\n",
    "- $\\pi(s, RIGHT) = \\sigma(s^\\top \\theta)$\n",
    "- $\\pi(s, LEFT) = 1 - \\sigma(s^\\top \\theta)$\n",
    "\n",
    "Then, let's compute the derivative of the sigmoid function:\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "1. Compute $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT})$:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT})\n",
    "&= \\nabla_\\theta \\log \\sigma(s^\\top \\theta) \\\\\n",
    "&= \\frac{1}{\\sigma(s^\\top \\theta)} \\nabla_\\theta \\sigma(s^\\top \\theta) \\\\\n",
    "&= \\frac{1}{\\sigma(s^\\top \\theta)} \\sigma(s^\\top \\theta) (1 - \\sigma(s^\\top \\theta)) ~ s \\\\\n",
    "&= (1 - \\sigma(s^\\top \\theta)) ~ s \\\\\n",
    "&= \\pi_\\theta(s, \\text{LEFT}) \\times s \\\\\n",
    "\\end{align}\n",
    "\n",
    "2. Compute $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT})$:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT})\n",
    "&= \\nabla_\\theta \\log (1 - \\sigma(s^\\top \\theta)) \\\\\n",
    "&= \\frac{1}{1 - \\sigma(s^\\top \\theta)} \\nabla_\\theta (1 - \\sigma(s^\\top \\theta)) \\\\\n",
    "&= \\frac{-1}{1 - \\sigma(s^\\top \\theta)} \\sigma(s^\\top \\theta) (1 - \\sigma(s^\\top \\theta)) ~ s \\\\\n",
    "&= -\\sigma(s^\\top \\theta) ~ s \\\\\n",
    "&= -\\pi_\\theta(s, \\text{RIGHT}) \\times s \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement REINFORCE\n",
    "\n",
    "Fill the following cells to implement the REINFORCE algorithm (defined in the introduction of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v0\"\n",
    "\n",
    "# Since the goal is to attain an average return of 195, horizon should be larger than 195 steps (say 300 for instance)\n",
    "EPISODE_DURATION = 300\n",
    "\n",
    "ALPHA_INIT = 0.1\n",
    "SCORE = 195.0\n",
    "NUM_EPISODES = 100\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `play_one_episode` function that plays an episode with the given policy $\\pi_\\theta$ (for fixed horizon $H$) and returns its rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode\n",
    "def play_one_episode(env, theta, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    s_t = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(s_t)\n",
    "\n",
    "    for t in range(max_episode_length):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        a_t = draw_action(s_t, theta)\n",
    "        s_t, r_t, done, info = env.step(a_t)\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        episode_states.append(s_t)\n",
    "        episode_actions.append(a_t)\n",
    "        episode_rewards.append(r_t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Implement the `score_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $H$) and returns:\n",
    "- `success`: `True` if the agent got an average reward greater or equals to 195 over 100 consecutive trials, `False` otherwise\n",
    "- `num_success`: the number of episodes where the agent got an average reward greater or equals to 195\n",
    "- `average_return`: the average reward on the `num_episodes` episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_on_multiple_episodes(env, theta, score=SCORE, num_episodes=NUM_EPISODES, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    \n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "    num_success = 0\n",
    "    average_return = 0\n",
    "    num_consecutive_success = [0]\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "        _, _, episode_rewards = play_one_episode(env, theta, max_episode_length, render)\n",
    "\n",
    "        total_rewards = sum(episode_rewards)\n",
    "\n",
    "        if total_rewards >= score:\n",
    "            num_success += 1\n",
    "            num_consecutive_success[-1] += 1\n",
    "        else:\n",
    "            num_consecutive_success.append(0)\n",
    "\n",
    "        average_return += (1.0 / num_episodes) * total_rewards\n",
    "\n",
    "        if render:\n",
    "            print(\"Test Episode {0}: Total Reward = {1} - Success = {2}\".format(episode_index,total_rewards,total_rewards>score))\n",
    "\n",
    "    if max(num_consecutive_success) >= 100:    # MAY BE ADAPTED TO SPEED UP THE LERNING PROCEDURE\n",
    "        success = True\n",
    "    else:\n",
    "        success = False\n",
    "        \n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return success, num_success, average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implement the `compute_policy_gradient` function that returns Policy Gradient for a given episode: policy gradient = $\\sum_{t=1}^H \\nabla_\\theta \\log \\pi_\\theta(s_t,a,_t) R_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Policy Gradient for a given episode\n",
    "def compute_policy_gradient(episode_states, episode_actions, episode_rewards, theta):\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    H = len(episode_rewards)\n",
    "    PG = 0\n",
    "\n",
    "    for t in range(H):\n",
    "\n",
    "        èè = logistic_regression(episode_states[t], theta)\n",
    "        a_t = episode_actions[t]\n",
    "        R_t = sum(episode_rewards[t::])\n",
    "        if a_t == LEFT:\n",
    "            g_theta_log_pi = - prob_push_right * episode_states[t] * R_t\n",
    "        else:\n",
    "            prob_push_left = (1 - prob_push_right)\n",
    "            g_theta_log_pi = prob_push_left * episode_states[t] * R_t\n",
    "\n",
    "        PG += g_theta_log_pi\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return PG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**: Implement the `train` function that updates $\\theta$ parameters with gradient ascent until the agent got an average reward greater or equals to 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent got an average reward greater or equals to 195 over 100 consecutive trials\n",
    "def train(env, theta_init, max_episode_length = EPISODE_DURATION, alpha_init = ALPHA_INIT):\n",
    "\n",
    "    theta = theta_init\n",
    "    episode_index = 0\n",
    "    average_returns = []\n",
    "\n",
    "    success, _, R = score_on_multiple_episodes(env, theta)\n",
    "    average_returns.append(R)\n",
    "\n",
    "    # Train until success\n",
    "    while (not success):\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Rollout\n",
    "        episode_states, episode_actions, episode_rewards = play_one_episode(env, theta, max_episode_length)\n",
    "\n",
    "        # Schedule step size\n",
    "        #alpha = alpha_init\n",
    "        alpha = alpha_init / (1 + episode_index)\n",
    "\n",
    "        # Compute gradient\n",
    "        PG = compute_policy_gradient(episode_states, episode_actions, episode_rewards, theta)\n",
    "\n",
    "        # Do gradient ascent\n",
    "        theta += alpha * PG\n",
    "\n",
    "        # Test new policy\n",
    "        success, _, R = score_on_multiple_episodes(env, theta, render=False)\n",
    "\n",
    "        # Monitoring\n",
    "        average_returns.append(R)\n",
    "\n",
    "        episode_index += 1\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\"Episode {0}, average return: {1}\".format(episode_index, R))\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "    return theta, episode_index, average_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(ENV_NAME)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "#env.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(1, dim)\n",
    "\n",
    "# Train the agent\n",
    "theta, i, average_returns = train(env, theta_init)\n",
    "\n",
    "print(\"Solved after {} iterations\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_on_multiple_episodes(env, theta, num_episodes=10, render=True)\n",
    "env.render_wrapper.make_gif(\"ex4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the average reward w.r.t. PG iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curve\n",
    "plt.plot(range(len(average_returns)),average_returns)\n",
    "plt.title(\"Average reward on 100 episodes\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: solve more challenging environments using Stable-Baselines\n",
    "\n",
    "Knowing main Reinforcement Learning concepts is important to solve RL problems, but being able to quickly solve problems reusing robust and proven implementations of RL algorithms is important too.\n",
    "\n",
    "Stable-Baselines is a Python library that provide state of the art ready to use RL algorithms.\n",
    "\n",
    "The following notebook provides an example of how to use it with Google Colab: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab7_rl3_reinforce_baselines.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
