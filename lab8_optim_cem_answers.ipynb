{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - CEM\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2021/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2021](https://moodle.polytechnique.fr/course/view.php?id=9352) Lab session #8\n",
    "\n",
    "2019-2021 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2021/master?filepath=lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2021/blob/master/lab8_optim_cem_answers.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2021/raw/master/lab8_optim_cem_answers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: OpenAI *Gym*, NumPy, Pandas, Seaborn and imageio.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gym[box2d] numpy pandas seaborn imageio\n",
    "``\n",
    "\n",
    "C.f. https://github.com/openai/gym#installing-everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install inf581\".split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "    from inf581 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like CEM are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for previous Reinforcement Learning labs, we will use standard problems provided by OpenAI Gym suite.\n",
    "Especially, we will try to solve the MountainCarContinuous-v0 problem (https://github.com/openai/gym/wiki/MountainCarContinuous-v0) which offers both continuous space and action states.\n",
    "\n",
    "An underpowered car must climb a one-dimensional hill to reach a target.\n",
    "The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates.\n",
    "On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty. The problem is considered solved if a reward of 90 is obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement CEM and test it on the CartPole environment\n",
    "\n",
    "Fill the following cells to implement the CEM algorithm (defined in the introduction of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the goal is to attain an average return of 195, horizon should be larger than 195 steps (say 300 for instance)\n",
    "EPISODE_DURATION = 900\n",
    "NUM_EPISODES = 10  # 100\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    return prob_push_right\n",
    "\n",
    "\n",
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "    r = np.random.rand()\n",
    "    if r < prob_push_right:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Generate an episode\n",
    "def eval_one_episode(theta, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    s_t = env.reset()\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    for t in range(max_episode_length):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        a_t = draw_action(s_t, theta)\n",
    "        s_t, r_t, done, info = env.step(a_t)\n",
    "\n",
    "        episode_rewards.append(r_t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return sum(episode_rewards)\n",
    "\n",
    "\n",
    "def eval_multiple_episodes(theta, num_episodes=NUM_EPISODES, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    \n",
    "    reward_list = []\n",
    "\n",
    "    for episode_index in range(num_episodes):\n",
    "        episode_global_reward = eval_one_episode(theta, max_episode_length, render)\n",
    "        reward_list.append(episode_global_reward)\n",
    "\n",
    "        if render:\n",
    "            print(\"Test Episode {}: Total Reward = {}\".format(episode_index, episode_global_reward))\n",
    "\n",
    "    return np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `cem_uncorrelated` function that updates $\\theta$ parameters with a Cross Entropy Method until the agent got an average reward greater or equals to 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(objective_function, mean_array, var_array,\n",
    "                     n_iterations=500, sample_size=50, elite_frac=0.2,\n",
    "                     print_every=10,\n",
    "                     hist_dict=None):\n",
    "    \"\"\"Cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        objective_function (function): the function to maximize\n",
    "        mean_array (array of floats): the initial proposal distribution (mean)\n",
    "        var_array (array of floats): the initial proposal distribution (variance)\n",
    "        n_iterations (int): number of training iterations\n",
    "        sample_size (int): size of population at each iteration\n",
    "        elite_frac (float): rate of top performers to use in update with elite_frac ∈ ]0;1]\n",
    "        print_every (int): how often to print average score\n",
    "        hist_dict (dict): logs\n",
    "    \"\"\"\n",
    "    assert 0. < elite_frac <= 1.\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    for iteration_index in range(0, n_iterations):\n",
    "\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (THETA VECTORS) ################\n",
    "\n",
    "        theta_array = np.random.multivariate_normal(mean=mean_array, cov=np.diag(var_array), size=(sample_size))\n",
    "\n",
    "        # EVALUATE SAMPLES AND EXTRACT THE BEST ONES (\"ELITE\") ################\n",
    "\n",
    "        score_array = np.array([objective_function(theta) for theta in theta_array])\n",
    "\n",
    "        sorted_indices_array = score_array.argsort()              # Sort from the lower score to the higher one\n",
    "        elite_indices_array = sorted_indices_array[-n_elite:]     # Here we wants to *maximize* the objective function thus we take the samples that are at the end of the sorted_indices\n",
    "\n",
    "        elite_theta_array = theta_array[elite_indices_array]\n",
    "\n",
    "        #df = pd.DataFrame(theta_array)\n",
    "        #df['elite'] = 0\n",
    "        #df.iloc[elite_indices_array, -1] = 1\n",
    "        #print(df)\n",
    "        #sns.pairplot(df, hue=\"elite\")\n",
    "        #plt.show()\n",
    "\n",
    "        # FIT THE NORMAL DISTRIBUTION ON THE ELITE POPULATION #################\n",
    "\n",
    "        mean_array = elite_theta_array.mean(axis=0)\n",
    "        var_array = elite_theta_array.var(axis=0)\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "        \n",
    "        if iteration_index % print_every == 0:\n",
    "            #print(\"Iteration {}\\tScore {}\\tMean: {}\\tVar: {}\".format(iteration_index, objective_function(mean_array), str(mean_array), str(var_array)))\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, objective_function(mean_array)))\n",
    "        \n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = [objective_function(mean_array)] + mean_array.tolist() + var_array.tolist()\n",
    "\n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = {}\n",
    "\n",
    "objective_function = eval_one_episode\n",
    "#objective_function = eval_multiple_episodes\n",
    "\n",
    "init_mean_array = np.zeros(4)\n",
    "init_var_array = np.ones(4) * 100.\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function, mean_array=init_mean_array, var_array=init_var_array,\n",
    "                         n_iterations=50, sample_size=50, elite_frac=0.1,\n",
    "                         print_every=1,\n",
    "                         hist_dict=hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"])\n",
    "df.score.plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "\n",
    "df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "RenderWrapper.register(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multiple_episodes(theta, num_episodes=5, render=True)\n",
    "env.render_wrapper.make_gif(\"ex1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a policy for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Parametric Stochastic Policy ################################################\n",
    "###############################################################################\n",
    "\n",
    "# Activation functions ########################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    x_and_zeros = np.array([x, np.zeros(x.shape)])\n",
    "    return np.max(x_and_zeros, axis=0)\n",
    "\n",
    "# Dense Multi-Layer Neural Network ############################################\n",
    "\n",
    "class NeuralNetworkPolicy:\n",
    "\n",
    "    def __init__(self, activation_functions, shape_list):\n",
    "        self.activation_functions = activation_functions\n",
    "        self.shape_list = shape_list\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        weights = unflatten_weights(theta, self.shape_list)\n",
    "\n",
    "        return feed_forward(inputs=state,\n",
    "                            weights=weights,\n",
    "                            activation_functions=self.activation_functions)\n",
    "\n",
    "\n",
    "\n",
    "def feed_forward(inputs, weights, activation_functions, verbose=False):\n",
    "    x = inputs.copy()\n",
    "    for layer_weights, layer_activation_fn in zip(weights, activation_functions):\n",
    "\n",
    "        y = np.dot(x, layer_weights[1:])\n",
    "        y += layer_weights[0]\n",
    "        layer_output = layer_activation_fn(y)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"x\", x)\n",
    "            print(\"bias\", layer_weights[0])\n",
    "            print(\"W\", layer_weights[1:])\n",
    "            print(\"y\", y)\n",
    "            print(\"z\", layer_output)\n",
    "\n",
    "        x = layer_output\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"...\")\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def weights_shape(weights):\n",
    "    return [weights_array.shape for weights_array in weights]\n",
    "\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\"Convert weight parameters to a 1 dimension array (more convenient for optimization algorithms)\"\"\"\n",
    "    nested_list = [weights_2d_array.flatten().tolist() for weights_2d_array in weights]\n",
    "    flat_list = list(itertools.chain(*nested_list))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def unflatten_weights(flat_list, shape_list):\n",
    "    length_list = [shape[0] * shape[1] for shape in shape_list]\n",
    "\n",
    "    nested_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    for length, shape in zip(length_list, shape_list):\n",
    "        nested_list.append(np.array(flat_list[start_index:start_index+length]).reshape(shape))\n",
    "        start_index += length\n",
    "\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Objective function ##########################################################\n",
    "###############################################################################\n",
    "\n",
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, env, policy, ndim, num_episodes=1, max_time_steps=float('inf')):\n",
    "        self.ndim = ndim\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "\n",
    "        self.num_evals = 0\n",
    "        self.hist = []\n",
    "        self.hist_policy = []\n",
    "\n",
    "    def eval(self, policy_params, num_episodes=None, render=False):\n",
    "        \"\"\"Evaluate a policy\"\"\"\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            total_rewards = 0.\n",
    "            state = self.env.reset()\n",
    "\n",
    "            for t in range(self.max_time_steps):\n",
    "                if render:\n",
    "                    self.env.render_wrapper.render()\n",
    "\n",
    "                action = self.policy(state, policy_params)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "            if render:\n",
    "                print(\"Test Episode {0}: Total Reward = {1}\".format(i_episode, total_rewards))\n",
    "\n",
    "        # Logs\n",
    "\n",
    "        if LOG_SCORES:\n",
    "            self.hist.append({\"eval\": self.num_evals,\n",
    "                              \"episode\": i_episode,\n",
    "                              \"total_rewards\": total_rewards})\n",
    "\n",
    "            if self.num_evals % LOG_RECORD_INTERVAL == 0:\n",
    "                with open(\"fobj_hist.json\", \"w\") as fd:\n",
    "                    json.dump(self.hist, fd)\n",
    "\n",
    "        if LOG_POLICIES:\n",
    "            self.hist_policy.append(policy_params.tolist())\n",
    "\n",
    "            if self.num_evals % LOG_RECORD_INTERVAL == 0:\n",
    "                with open(\"fobj_hist_policy.json\", \"w\") as fd:\n",
    "                    json.dump(self.hist_policy, fd)\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\"Avg total Reward = {:0.3f}, Theta = {}\".format(average_total_rewards, policy_params))\n",
    "\n",
    "        return average_total_rewards   # Optimizers do minimization by default...\n",
    "\n",
    "    def __call__(self, policy_params):\n",
    "        return self.eval(policy_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: solve the LunarLander problem (continuous version) with CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** read https://gym.openai.com/envs/LunarLanderContinuous-v2/ and https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2 to discover the LunarLanderContinuous environment.\n",
    "\n",
    "**Notice:** A reminder of Gym main concepts is available at https://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the MountainCar environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the LunarLander environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = np.array([1., 1.])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex2left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = np.array([-1., -1.])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex2right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the LunarLander environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(3):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(100):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE_DURATION = 1500\n",
    "NUM_EPISODES_PER_EVAL = 1\n",
    "\n",
    "VERBOSE = False\n",
    "LOG_SCORES = True\n",
    "LOG_POLICIES = False\n",
    "LOG_RECORD_INTERVAL = 1000\n",
    "\n",
    "GYM_ENVIRONMENT = \"MountainCarContinuous-v0\"\n",
    "SUCCESS_SCORE = 90\n",
    "\n",
    "###############################################################################\n",
    "# Main ########################################################################\n",
    "###############################################################################\n",
    "\n",
    "env = gym.make(GYM_ENVIRONMENT)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation_space_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(observation_space_dim)\n",
    "init_mean_array = np.zeros(65)\n",
    "init_var_array = np.ones(65) * 1000.\n",
    "\n",
    "# Make a neural network with 1 hidden layer of 16 units\n",
    "weights = (np.zeros([env.observation_space.shape[0] + 1, 16]),\n",
    "           np.zeros([17, env.action_space.shape[0]]))\n",
    "\n",
    "# Set the neural network activation functions (one function per layer)\n",
    "activation_functions = (relu, tanh)\n",
    "\n",
    "flat_weights_list = flatten_weights(weights)\n",
    "num_params = len(flat_weights_list)\n",
    "print(\"Number of parameters (neural network weights) to optimize:\", num_params)\n",
    "w_shape = weights_shape(weights)\n",
    "print(\"Number of parameters per layer:\", w_shape)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(activation_functions, w_shape)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       ndim=num_params,  # number of dimensions of the parameter (weights) space\n",
    "                                       num_episodes=NUM_EPISODES_PER_EVAL,\n",
    "                                       max_time_steps=MAX_EPISODE_DURATION)\n",
    "\n",
    "# Optimization ############################################################\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function, mean_array=init_mean_array, var_array=init_var_array,\n",
    "                         n_iterations=50, sample_size=50, elite_frac=0.2,\n",
    "                         print_every=1,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "print(\"Solved after {} evaluations\".format(objective_function.num_evals))\n",
    "print(\"Optimized weights: \", theta)\n",
    "\n",
    "# Test final policy\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "df.iloc[:,0].plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", figsize=(30, 25))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(30, 25))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(GYM_ENVIRONMENT)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "# Test final policy\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: implement SAES and solve the LunarLander problem (continuous version) with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 1: solve the MountainCar environment (continuous version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** read  https://gym.openai.com/envs/MountainCarContinuous-v0/ and https://github.com/openai/gym/wiki/MountainCarContinuous-v0 to discover the MountainCar environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** A reminder of Gym main concepts is available at https://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the MountainCar environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the MountainCar environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = np.array([1.])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex2left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = np.array([-1.])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex2right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the MountainCar environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(3):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(100):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Solve the MounainCar problem with CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE_DURATION = 1500\n",
    "NUM_EPISODES_PER_EVAL = 1\n",
    "\n",
    "VERBOSE = False\n",
    "LOG_SCORES = True\n",
    "LOG_POLICIES = False\n",
    "LOG_RECORD_INTERVAL = 1000\n",
    "\n",
    "GYM_ENVIRONMENT = \"MountainCarContinuous-v0\"\n",
    "SUCCESS_SCORE = 90\n",
    "\n",
    "###############################################################################\n",
    "# Main ########################################################################\n",
    "###############################################################################\n",
    "\n",
    "env = gym.make(GYM_ENVIRONMENT)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation_space_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(observation_space_dim)\n",
    "init_mean_array = np.zeros(65)\n",
    "init_var_array = np.ones(65) * 1000.\n",
    "\n",
    "# Make a neural network with 1 hidden layer of 16 units\n",
    "weights = (np.zeros([env.observation_space.shape[0] + 1, 16]),\n",
    "           np.zeros([17, env.action_space.shape[0]]))\n",
    "\n",
    "# Set the neural network activation functions (one function per layer)\n",
    "activation_functions = (relu, tanh)\n",
    "\n",
    "flat_weights_list = flatten_weights(weights)\n",
    "num_params = len(flat_weights_list)\n",
    "print(\"Number of parameters (neural network weights) to optimize:\", num_params)\n",
    "w_shape = weights_shape(weights)\n",
    "print(\"Number of parameters per layer:\", w_shape)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(activation_functions, w_shape)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       ndim=num_params,  # number of dimensions of the parameter (weights) space\n",
    "                                       num_episodes=NUM_EPISODES_PER_EVAL,\n",
    "                                       max_time_steps=MAX_EPISODE_DURATION)\n",
    "\n",
    "# Optimization ############################################################\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function, mean_array=init_mean_array, var_array=init_var_array,\n",
    "                         n_iterations=50, sample_size=50, elite_frac=0.2,\n",
    "                         print_every=1,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "print(\"Solved after {} evaluations\".format(objective_function.num_evals))\n",
    "print(\"Optimized weights: \", theta)\n",
    "\n",
    "# Test final policy\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "df.iloc[:,0].plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", figsize=(30, 25))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(30, 25))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(GYM_ENVIRONMENT)\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "# Test final policy\n",
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 2: test the CMAES algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
